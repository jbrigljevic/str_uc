{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b432f495",
   "metadata": {},
   "source": [
    "Ovo će biti bilježnica s praktičnom realizacijom naših rješenja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb06f7c",
   "metadata": {},
   "source": [
    "# Opće uvjetovanosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95181ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skini(url: str, datoteka: str):\n",
    "    import urllib.request as rq, pathlib\n",
    "    with rq.urlopen(url) as konekcija:\n",
    "        pathlib.Path(datoteka).write_bytes(konekcija.read())\n",
    "        \n",
    "test_podaci = 'https://raw.githubusercontent.com/jbrigljevic/str_uc/main/Podaci/test.csv'\n",
    "train_podaci = 'https://raw.githubusercontent.com/jbrigljevic/str_uc/main/Podaci/train.csv'\n",
    "skini(test_podaci, 'test_podaci.csv')\n",
    "skini(train_podaci, 'train_podaci.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1e5c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_podaci.csv')\n",
    "train_data = pd.read_csv('train_podaci.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf66b47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33526621",
   "metadata": {},
   "source": [
    "# k-nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1711b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Juraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\Juraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Juraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Juraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Juraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61a32c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train nakon micanja stop riječi\n",
      "0           deed reason earthquake may allah forgive u\n",
      "1                forest fire near la ronge sask canada\n",
      "2    resident asked shelter place notified officer ...\n",
      "3    people receive wildfire evacuation order calif...\n",
      "4    got sent photo ruby alaska smoke wildfire pour...\n",
      "5    rockyfire update california hwy closed directi...\n",
      "6    flood disaster heavy rain cause flash flooding...\n",
      "7                               top hill see fire wood\n",
      "8    emergency evacuation happening building across...\n",
      "9                           afraid tornado coming area\n",
      "Name: text, dtype: object\n",
      "df_test nakon micanja stop riječi\n",
      "0                          happened terrible car crash\n",
      "1    heard earthquake different city stay safe ever...\n",
      "2    forest fire spot pond goose fleeing across str...\n",
      "3                 apocalypse lighting spokane wildfire\n",
      "4                   typhoon soudelor kill china taiwan\n",
      "5                                   shaking earthquake\n",
      "6     probably still show life arsenal yesterday eh eh\n",
      "7                                                  hey\n",
      "8                                             nice hat\n",
      "9                                                 fuck\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train = train_data\n",
    "df_test = test_data\n",
    "\n",
    "s = stopwords.words('english')\n",
    "#dodatne stop riječi\n",
    "s.extend(['today', 'tomorrow', 'outside', 'out', 'there'])\n",
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df_train.loc[i,'text'])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "    review = ' '.join(review)\n",
    "    df_train.loc[i, 'text'] = review\n",
    "    \n",
    "X_train = df_train['text']\n",
    "y_train = df_train['target']\n",
    "\n",
    "print(\"df_train nakon micanja stop riječi\")\n",
    "print(df_train['text'][:10])\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df_test.loc[i,'text'])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "    review = ' '.join(review)\n",
    "    df_test.loc[i, 'text'] = review\n",
    "    \n",
    "X_test = df_test['text']\n",
    "\n",
    "print(\"df_test nakon micanja stop riječi\")\n",
    "print(df_test['text'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60bbea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary of features and transforms documents to feature vectors and convert our text documents to a\n",
    "# matrix of token counts (CountVectorizer)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "# transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a1628094",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf = knn.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Input Data to predict their classes of the given categories\n",
    "primjer = [X_test[0]]\n",
    "# building up feature vector of our input\n",
    "X_new_counts = count_vect.transform(primjer)\n",
    "# We call transform instead of fit_transform because it's already been fit\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "277aba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3451eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n"
     ]
    }
   ],
   "source": [
    "# We can use Pipeline to add vectorizer -> transformer -> classifier all in a one compound classifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', knn),\n",
    "])\n",
    "# Fitting our train data to the pipeline\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting our test data\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac5c12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export u csv\n",
    "dict = {\"id\":list(df_test[\"id\"]), \"target\":predicted}\n",
    "df_test_export = pd.DataFrame(dict)\n",
    "df_test_export.set_index(\"id\", inplace = True)\n",
    "retci = [[\"id\", \"target\"]]\n",
    "for i in range(len(df_test_export.index)):\n",
    "    retci.append([df_test_export.index[i], predicted[i]])\n",
    "\n",
    "np.savetxt(\"test_result.csv\", retci, delimiter =\",\", fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e5946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
